Documentation

1.	Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it.  As part of your answer, give some background on the dataset and how it can be used to answer the project question.  Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]

The goal of this project is to use machine learning to detect fraudsters from the Enron dataset.  The Enron dataset contains a large quantity of emails between Enron employees along with data about employee financial compensation, e.g. salary, bonuses, etc. Each person of interest is labelled so we have a supervised learning problem.  The Enron data set contains 146 persons, 18 of which are poi's.  Each person in the dataset has 18 features.  Across the total dataset, the number of NaN's for each feature are:

{'bonus': 64,
 'deferral_payments': 107,
 'deferred_income': 97,
 'director_fees': 129,
 'email_address': 35,
 'exercised_stock_options': 44,
 'expenses': 51,
 'from_messages': 60,
 'from_poi_to_this_person': 60,
 'from_this_person_to_poi': 60,
 'loan_advances': 142,
 'long_term_incentive': 80,
 'other': 53,
 'restricted_stock': 36,
 'restricted_stock_deferred': 128,
 'salary': 51,
 'shared_receipt_with_poi': 60,
 'to_messages': 60,
 'total_payments': 21,
 'total_stock_value': 20}

 Outlier's were investigated visually using a pairwise scatter plot matrix leading to the summary statistic TOTAL being removed from the dataset.


2.	What features did you end up using in your POI identifier, and what selection process did you use to pick them?  Did you have to do any scaling?  Why or why not?  As part of the assignment, you should attempt to engineer your own feature that doesn’t come ready-made in the dataset--explain what feature you tried to make, and the rationale behind it.  (You do not necessarily have to use it in the final analysis, only engineer and test it.)  If you used an algorithm like a decision tree, please also give the feature importances of the features that you use.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]

Create new features:
Principle component analysis was explored to create new features from the existing data.  Ultimately, however, PCA was not used to obtain the best result in terms of accuracy, precision and recall. 

Intelligently select features:
Financial features from the dataset were manually selected.  For financial features where the total count of NaNs for a feature was greater than 60% of the total data points, this feature was excluded from the assessment.  Finally, selectKBest was used to select the most relevant features with k=4.

Properly scale features:
Financial features were scaled using the MinMaxScaler function


3.	What algorithm did you end up using?  What other one(s) did you try? [relevant rubric item: “pick an algorithm”]
A Random Forest Classifier, with 50 estimators and a maximum of 3 features, was used to obtain the best result with a pipeline including MinMaxScaler and SelectKBest using chi2 and k=50.  If performance is an issue, KNeighborsClassifier is much faster (~1.5s vs 24s)


4.	What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm?  (Some algorithms don’t have parameters that you need to tune--if this is the case for the one you picked, identify and briefly explain how you would have done it if you used, say, a decision tree classifier). [relevant rubric item: “tune the algorithm”]

Tune the algorithm:
Each classifier explored in the assessment has one or more parameters related to the structure of the algorithm.  By 'tuning' these are likely to influence the score of our classifier based on our selected validation metrics.  So it is important to explore how changes to the initial parameter assumptiosn affect the score.  One side-effect can also be computation time so it is important to understand how tuning the parameters effects computation speed.


5.	What is validation, and what’s a classic mistake you can make if you do it wrong?  How did you validate your analysis?  [relevant rubric item: “validation strategy”]

Validation strategy:
Validation involves training our model and exploring how good this trained model is, in terms of how good it is at providing the correct prediction, when we throw new data at it.  


6.	Give at least 2 evaluation metrics, and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]

Three performance metrics were explored in the model validation include:

	* accuracy = 0.865  
		- the percentage of poi's that were correctly identified (true positives / total people)
	* precision = 0.492 
		- when our model predicts a poi, how likely is it that this prediciton is correct (true positives/(true positives + false positives))
	* recall = 0.385
		- the likelyhood that our model correctly predicts poi's (true positives/(true positives + false negatives))


As mentioned above, KNeighborsClassifier could be used if performance is an issue as it runs in about 5% of the time with the following performance metrics:

	* accuracy = 0.836, 
	* precision = 0.385, 
	* recall = 0.383.